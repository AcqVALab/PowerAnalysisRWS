
![Introduction to Power Analysis with R - AcqVA Aurora workshop](https://slcladal.github.io/images/acqvalab.png)


# Introduction

This tutorial introduces power analysis for determining sample size using R with the aim of informing about the theoretical underpinnings, showcasing how to perform power analysis, and enabling participants on how to adapt, change, and modify data sets so that they can be fed into a power analysis using the `simr` (Green and MacLeod 2016a). 



This tutorial is aimed at intermediate users of R. The aim is not to provide a fully-fledged guide but rather to show and exemplify how to perform a power analysis for a generalized linear mixed-effects model as this is the most common type of analysis for experimental studies.


<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
The entire R Notebook for the tutorial can be downloaded  via https://github.com/MartinSchweinberger/AcqVA_PowerR_WS/acqvapwrws.Rmd.  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file (see https://github.com/MartinSchweinberger/AcqVA_DataVisR_WS/bibliography.bib) and store it in the same folder where you store the Rproj file.<br></p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>



## What to do before the workshop

To get the most out of this workshop, you will need to have some (basic) R skills and (basic) knowledge of how to work with R, RStudio, R Projects, and R Notebooks. If you have no or little experience with this or if you need to refresh your skills, please carefully read (or optimally go through) these tutorials:

* [Getting started with R](https://slcladal.github.io/intror.html)


Before attending the workshop, you need to install the following packages in RStudio:

* `here` (for easy pathing)

* `tidyverse` (for data processing)

* `pwr` (for simple power analyses)

* `lme4` (for mixed-models)

* `sjPlot` (for summaries of mixed-models)

* `report` (for reporting mixed-models)

* `simr` (for power analyses for glmms)

* `knitr` (for knitting R Notebooks)

* `markdown` (for rendering Rmd files)

* `rmarkdown`  (for R Markdown formatting)

* `installr` (for updating R)


You can update R and install these packages by clicking on `packages` on the top of the lower right pane in RStudio or you can execute the following code in the lower left, `Console` pane of RStudio. 

```{r install, eval = F, message=F, warning=F}
# update R
#install.packages("installr")
#library(installr)
#updateR()
# install required packages
install.packages("devtools")
library("devtools")
install_version("nloptr", version = "1.2.1", repos = "http://cran.us.r-project.org")
install.packages(c("tidyverse", "pwr", "sjPlot", "lme4", "simr"))
```


**It is really important that you have knowledge of R and RStudio and that you have installed the packages before the workshop so that we do not have to deal with technical issues too much.**

You can find a more detailed version of the content of this workshop in these two LADAL tutorials

* [Power Analysis with R](https://slcladal.github.io/pwr.html)

You can follow this workshop in different ways - you can sit back and watch it like a lecture or take a more active role - that said, the intention for this workshop is clearly to be practical so that I show something and then you do it on you computer and we have exercises where you can try out what you have just learned.

So, in essence, there are the following three options for following this workshop:

1. You can sit back and enjoy and focus on what you see and then go back home and try it by yourself later.

2. You can follow this workshop in RStudio and execute code, see what it does, understand it, and adapt it. This requires some skills in R and RStudio - although I have trued to keep things simple.

3. You can use the Jupyter Notebook on Google Colab to follow the workshop (and you are then ready to go - you only  have to install the packages which takes a couple of minutes). You can then copy that Notebook and save it in your own Google Drive and then execute code, modify it, and understand it. This does require less skills and will be easier for people who just want to focus on the code that produces certain visualizations without doing it in RStudio. Also, this get rid of most technical issues (hopefully) but the installation of packages will take a while and you will need to re-install the packages every time you want to re-use the Jupyter Notebook.

Choose which option suits you best and then go with it. 

## Timeline

Here is what we have planned to cover in this workshop:

Tuesday, February 1, 10-12am

* Introduction

* What is statistical power?

* Why perform power analyses?

* Session preparation 

* Basic Power Analysis (`pwr`)


Tuesday, February 1, 1-3pm

* Advanced power analyses

* Power analysis for piloted data (`simr`)

* Linear Mixed-Effects Model

* Linear Mixed-Effects Model

* Post-hoc power analyses 

* Setting effect sizes

* Wrap-up and Resources



# Getting started

If you choose option 2, you need to set up our R session and prepare our R project at the very beginning of the workshop. 

For everything to work, please do the following:

* Create a folder for this workshop somewhere on your computer, e.g. called *AcqVA_DataVisR_WS*

* In that folder, create two subfolders called *data* and *images*

* Open RStudio, go to File > New Project > Existing Directory (Browse to project folder) > Create (and hit Enter)

This will then create an R Project in the project folder.


## Theoretical underpinning


What is power and why do we need power analyses?

Let's have a look at some distributions to understand what is going on...

## Effect size

And let's start with varying effect size.


![2 groups from 1 distribution, N = 30](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist01.png)


Now, lets have a look at at the distribution of two different groups (group has a weak effect, Cohen's *d* = .2)


![2 groups from 2 distribution (weak effect, N = 30)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist02.png)

Now, lets have a look at at the distribution of two different groups (group has a medium effect, Cohen's *d* = .5)

![2 groups from 2 distribution (medium effect, N = 30)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist03.png)




Now, lets have a look at at the distribution of two different groups (group has a strong effect, Cohen's *d* = .8)

![2 groups from 2 distribution (strong effect, N = 30)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist04.png)



Now, lets have a look at at the distribution of two different groups (group has a very strong effect, Cohen's *d* = 1.2)

![2 groups from 2 distribution (very strong effect, N = 30)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist05.png)


**If variability and sample size remain constant, larger effects are easier to detect than smaller effects!**

## Sample size

And let's now look at sample size.

![2 groups from 2 distribution (medium effect, N = 30)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist06.png)




Let us now increase the sample size to N = 50.

![2 groups from 2 distribution (medium effect, N = 50)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist07.png)



**If variability and effect size remain constant, effects are easier to detect with increasing sample size!**

## Variability

And let's now look at variability.

![2 groups from 2 distribution (medium effect, N = 30, SD = 10)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist08.png)


Let's decrease the variability to sd = 5.

![2 groups from 2 distribution (medium effect, N = 30, SD = 5)](https://raw.githubusercontent.com/MartinSchweinberger/AcqVAPowerRWS/main/images/dist09.png)


**If the sample and effect size remain constant, effects are easier to detect with decreasing variability!**

**In summary**, there are three main factors that determine if a model finds an effect. The accuracy (i.e., the probability of finding an effect):

* the size of the effect (bigger effects are easier to detect)
* the variability of the effect (less variability makes it easier to detect an effect), and 
* the sample size (the bigger the sample size, the easier it is to detect an effect); 
  + number of subjects/participants
  + number of items/questions
  + number of observations per item within subjects/participants
  
Now, if a) we dealing with a very big effect, then we need only few participants and few items to accurately find this effect.

Or b) if we dealing with an effect that has low variability (it is observable for all subjects with the same strength), then we need only few participants and few items to accurately find this effect.



<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
What power analysis allow us to do is to calculate at what sample size we would be able to detect an effect of a certain strength with a predefined certainty, given a certain amount of variability. <br><br>In other words, assuming the variability in the pilot data is the same as in the population, how many subjects do we need to detect a weak (or median or strong) effect with 80 percent certainty?<br></p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>

# Today's Data


We will use 1 data set that is based on a two data set that was send in by Isabel. 


Here are summaries of the data sets:

1. **`AJT_V2`** (we will call this data `regdat`: contains 480 observations of the following 5 variables:
  + ID: 20 levels (0% missing)
  + Sentence: 24 levels
  + Group: 2 levels, namely *English* (n = 240) and *NorwegianEnglish* (n = 240)
  + WordOrder: 2 levels, namely *V2* (n = 240) and *V3* (n = 240)
  + SentenceType: 2 levels, namely *NonSubjectInitial* (n = 240) and *SentenceAdverbials* (n = 240)


**To upload the data into Google Colab, do the following**

To be able to load data into Google Colab, you need to click on the folder symbol to the left of the screen:

![Colab Folder Symbol](https://slcladal.github.io/images/ColabFolder.png)

Then on the upload symbol. 

![Colab Upload Symbol](https://slcladal.github.io/images/ColabUpload.png)


Next, upload the files you want to use. When you then execute the code (like to code chunk below, you will load the data into this notebook.


```{r loadown, echo = T, eval = F, message=FALSE, warning=FALSE}
regdat <- readRDS("regdat.rda")
# inspect
regdat
```

To apply the code and functions below to your own data, you will need to modify the code chunks and replace the data we use here with your own data object. 

# Basic Power Analysis

Let's start with a simple power analysis to see how power analyses work for simpler or basic statistical tests such as t-test, $\chi$^2^-test, or linear regression.

The `pwr` package  (Champely 2020) implements power analysis as outlined by Cohen (1988) and  allows to perform power analyses for the following tests (selection):

* balanced one way ANOVA (`pwr.anova.test`) 

* chi-square test (`pwr.chisq.test`)

* Correlation (`pwr.r.test`)

* general linear model (`pwr.f2.test`)
 	
* paired (one and two sample) t-test (`pwr.t.test`)

* two sample t-test with unequal N (`pwr.t2n.test`)

For each of these functions, you enter three of the four parameters (effect size, sample size, significance level, power) and the fourth is calculated. 

So how does this work? Have a look at the code chunk below.

## Power Analysis for One-Way ANOVAs

Let check how to calculate the necessary sample size for each group for a one-way ANOVA that compares 5 groups and that has a power of 0.80 (80 percent), when the effect size is moderate (*f* = 0.25) and the significance level is 0.05 (5 percent). Regarding the effect size *f*, *f* has been categorized as small = .1, medium .25, and large $≥$ 0.4 (see Cohen 1988).

```{r bp01, warning=F, message=F}
# load package
library(pwr)
# calculate minimal sample size
pwr.anova.test(k=5,            # 5 groups are compared
               f=.25,          # moderate effect size
               sig.level=.05,  # alpha/sig. level = .05
               power=.8)       # confint./power = .8
```

In this case, the minimum number of participants in each group would be 40.

Let's check how we could calculate the power if we had already collected data (with 30 participants in each group) and we want to report the power of our analysis (and et us assume that the effect size was medium).

```{r bp02, warning=F, message=F}
# calculate minimal sample size
pwr.anova.test(k=5,            # 5 groups are compared
               f=.25,          # moderate effect size
               sig.level=.05,  # alpha/sig. level = .05
               n=30)           # n of participants
```

In this case our analysis would only have a power or 66.8 percent. This means that we would only detect a medium sized effect in 66.7 percent of cases (which is considered insufficient).


## Power Analysis for GLMs

When determining the power of a generalized linear model, we need to provide

* the degrees of freedom for numerator (´u´)

* the degrees of freedom for denominator (`v`)

* the effect size (the estimate of the intercept and the slope/estimates of the predictors)

* the level of significance (i.e.,  the type I error probability)

The values of the parameters in the example below are adapted from the fixed-effects regression example that was used to analyze different teaching styles (see [here](https://slcladal.github.io/regression.html#Example_2:_Teaching_Styles)).

The effect size used here is $f^2$ that has be categorized as follows (see Cohen 1988): small $≥$ 0.02, medium $≥$ 0.15, and large $≥$ 0.35. So in order to determine if the data is sufficient to find a weak effect when comparing 2 groups with 30 participants in both groups (df_numerator_: 2-1; df_denominator_: (30-1) + (30-1)) and a significance level at $\alpha$ = .05, we can use the following code.


```{r pwrglm}
# general linear model
pwrglm <- pwr.f2.test(u = 1,
                      v = 58,
                      f2 = .02,
                      sig.level = 0.05)
# inspect results
pwrglm
```


The results show that the regression analyses used to evaluate the effectiveness of different teaching styles only had a power of `r pwrglm$power`.


## Power Analysis for t-tests

For t-tests (both paired and 2-sample t-tests), the effect size measure is Cohen's $d$ that has be categorized as follows (see Cohen 1988): small $≥$ 0.2, medium $≥$ 0.5, and large $≥$ 0.8. 

**Paired t-test**

So in order to determine if the data is sufficient to find a weak effect when comparing the pre- and post-test results of a group with 30 participants, evaluating an undirected hypothesis (thus the *two-tailed* approach), and a significance level at $\alpha$ = .05, we can use the following code.

```{r pwrpt}
# paired t-test
pwrpt <- pwr.t.test(d=0.2,
                    n=30,
                    sig.level=0.05,
                    type="paired",
                    alternative="two.sided")
# inspect
pwrpt
```


Given the data, a weak effect in this design can only be detected with a certainty of `r pwrpt$power` percet. This means that we would need to substantively increase the sample size to detect a small effect with this design.

**Two-sample (independent) t-test**

The power in a similar scenario but with two different groups (with 25 and 35 subjects) can be determined as follows (in this case we test a directed hypothesis that checks if the intervention leads to an increase in the outcome - hence the *greater* in the `alternative` argument):

```{r pwrit}
# independent t-test
pwr2t <- pwr.t2n.test(d=0.2,
                      n1=35,
                      n2 = 25,
                      sig.level=0.05,
                      alternative="greater")
# inspect
pwr2t
```

Given the data, a weak effect in this design can only be detected with a certainty of `r pwr2t$power` percent. This means that we would need to substantively increase the sample size to detect a small effect with this design.


## Power Analysis for $\chi$^2^-tests

Let us now check the power of a $\chi^2$^-test. For $\chi^2$^-test, the effect size measure used in the power analysis is $w$ that has be categorized as follows (see Cohen 1988): small $≥$ 0.1, medium $≥$ 0.3, and large $≥$ 0.5. Also, keep in mind that for $\chi^2$^-tests, at least 80 percent of cells need to have values $≥$ 5 and none of the cells should have expected values smaller than 1 (see Bortz, Lienert, and Boehnke 1990).. 



```{r pwrchi}
# x2-test
pwrx2 <- pwr.chisq.test(w=0.2,
                        N = 25, # total number of observations
                        df = 1,
                        sig.level=0.05)
# inspect
pwrx2
```


Given the data, a weak effect in this design can only be detected with a certainty of `r pwrx2$power` percent. This means that we would need to substantively increase the sample size to detect a small effect with this design.

**EXERCISE**

For the tests above (anova, glm, paired and independent t-test, and the $\chi^2$-test), how many participants would you need to have a power of 80 percent?

Here are the commands we used to help you:

* ANOVA: `pwr.anova.test(k=5, f=.25, sig.level=.05, n=30) `

* GLM: `pwr.f2.test(u = 1, v = 58, f2 = .02, sig.level = 0.05)`

* paired t-test: `pwr.t.test(d=0.2, n=30, sig.level=0.05, type="paired", alternative="two.sided")`

* independent t-test: `pwr.t2n.test(d=0.2,n1=35, n2 = 25, sig.level=0.05, alternative="greater")`

* $\chi^2$-test: `pwr.chisq.test(w=0.2, N = 25,  df = 1, sig.level=0.05)`

```{r expwr1, eval = F}



```



# Advanced Power Analysis

The basis for the present tutorial is Green and MacLeod (2016b) (which you can find [here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504)). Green and MacLeod (2016b) is a highly recommendable and thorough tutorial on performing power analysis in R. Recommendable literature on this topic are, e.g. Arnold et al. (2011)  and Johnson et al. (2015)  and [this tutorial](https://www.journalofcognition.org/articles/10.5334/joc.10/). 



Before we conduct a study, we should figure out, what sample we need to detect a small/medium effect with medium variability so that our model is sufficient to detect this kind of effect. In order to do this, we would generate a data set that mirrors the kind of data that we expect to get (with the properties that we expect to get). We can then fit a model to this data and check if a model would be able to detect the expected effect. However, because a single model does not tell us that much (ift could simply be luck that it happened to find the effect), we run many different models on variations of the data and see how many of them find the effect. As a general rule of thumb, we want a data set that allows a model to find a medium sized effect with at least an accuracy of 80 percent (Field et al. 2007).

In the following, we will go through how to determine what sample size we need for an example analysis.


## Preparation and session set up

As we we have installed the packages, we can activate them as shown below.

```{r prep2, message=F, warning=F} 
# load packages
library(tidyverse)
library(lme4)
library(sjPlot)
library(simr)
```

Once you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.

## Using piloted data and a lmer

Load data

```{r reg01, message=FALSE, warning=FALSE}
# load data
regdat <- readRDS("regdat.rda")
# inspect
head(regdat, 10);str(regdat)
```


Check how many levels we have.

```{r reg02, message=FALSE, warning=FALSE}
table(regdat$Group, regdat$WordOrder); 
table(regdat$Group, regdat$SentenceType); 
table(regdat$ID)
table(regdat$Sentence)
```

We could also add a response variable (but not now ;) )


### Generating the model

Set effects and specify model parameters

* fixed effects (and intercept)

* random effects

* variance

```{r reg03, message=F, warning=F}
# Intercept + slopes for fixed effects 
# (Group, Sentence_type, Word_order, and an interaction between Group * Sentence_type)
fixed <- c(.52, .52, .52, .52, .52)
# Random intercepts for Sentence and ID
#rand <- list(0.5, 0.1)
rand <- list(0.5)
# res. variance
res <- 2
```

Generate model

```{r reg04, message=F, warning=F}
#model <- makeLmer(y ~ (1|Sentence) + (1|ID) + Group * Sentence_type + Word_order, fixef=fixed, VarCorr=rand, sigma=res, data=regdat)
m1 <- makeLmer(y ~ (1|Sentence) + Group * SentenceType + WordOrder, 
               fixef=fixed, 
               VarCorr=rand, 
               sigma=res, 
               data=regdat)
# inspect
m1
```

Inspect summary table

```{r reg05}
sjPlot::tab_model(m1)
```

**The summary table shows that the effects are correct but none of them are reported as being significant!**

### Power analysis

Let us now check if the data is sufficient to detect the interaction between `Group` and `Sentence_type`.

```{r reg06, message=F, warning=F}
sim_wo <- simr::powerSim(m1, 
                        nsim=10,
                        test = fcompare(y ~ WordOrder))
# inspect results
sim_wo
```

**The data is sufficient to detect a weak effect of `WordOrder` with `r sim_wo$x` percent accuracy**

**Why did I set the estimates to 0.52?**

Let us now inspect the effect sizes and see why I used 0.52 as an effect size in the model.

```{r reg08, message=FALSE, warning=FALSE}
# extract fixed effect estimates
estimatesfixedeffects <- fixef(m1)
# convert estimates into odds ratios
exp(estimatesfixedeffects)
```

We will now check the effect size which can be interpreted according to Chen, Cohen, and Chen (2010; see also Cohen 1988).

* small effect (Cohen's d 0.2, OR = 1.68)

* medium effect (Cohen's d 0.5, OR = 3.47)

* strong effect  (Cohen's d 0.8, OR = 6.71)

### Power analysis for interactions

Power Analysis: let's check if the data set has enough power to detect a weak effect for `Group * SentenceType`.

```{r reg10, message=F, warning=F}
sim_gst <- simr::powerSim(m1, 
                          nsim=100, 
                          test = fcompare(y ~ Group * SentenceType))
# inspect
sim_gst
```

**The data is not sufficient to detect a weak effect of `Group * SentenceType` with `r sim_gst$x` percent accuracy**


## Extending Data

We will now extend the data to see how many participants and sentences you need.


### Adding sentences

```{r ext01, message=F, warning=F}
m1_as <- simr::extend(m1,
                      along="Sentence", 
                      n=50)
# inspect model
m1_as
```

Check power when using 60 sentences

```{r ext03, message=F, warning=F}
sim_m1_as_gst <- powerSim(m1_as, 
                   nsim=20, 
                   test = fcompare(y ~ Group * SentenceType))
# inspect
sim_m1_as_gst
```


Plot power curve

```{r ext04, results='hide', message = FALSE, warning=F}
pcurve_m1_as_gst <- simr::powerCurve(m1_as, 
                               test=fcompare(y ~ Group * SentenceType), 
                               along="Sentence",
                               nsim = 20, 
                               breaks=seq(10, 50, 10))
# show plot
plot(pcurve_m1_as_gst)
```


### Checking participants

What about increasing the number of participants?

```{r ext05, message=F, warning=F}
m2 <- makeLmer(y ~ (1|ID) + Group * SentenceType + WordOrder, 
               fixef=fixed, 
               VarCorr=rand, 
               sigma=res, 
               data=regdat)
# inspect
sjPlot::tab_model(m2)
```


Power analysis

```{r ext08, message=F, warning=F}
sim_m2_wo <- powerSim(m2, 
                   nsim=10, 
                   test = fcompare(y ~ WordOrder))
# inspect
sim_m2_wo
```

An alternative way to perform the simulation

```{r ext09, message=F, warning=F}
# set seed for replicability
set.seed(12345)
# perform power analysis for present model
rsim_m2_wo <- powerSim(m2, fixed("WordOrderV3", "z"), nsim=10)
# inspect
rsim_m2_wo
```

Plotting the power curve.

```{r ext11, results='hide', message = FALSE, warning=F}
pcurve_m2_wo <- simr::powerCurve(m2, 
                               test=fcompare(y ~ WordOrder),
                               along = "ID", 
                               nsim=10)
# show plot
plot(pcurve_m2_wo)
```

## Generating data and a glmer

In order to perform a  power analysis, we will start by loading the tidyverse package to process the data and by generating a data that we will use to determine the power of a regression model.

This simulated data set has

* 200 data points
* 2 Conditions (Control, Test)
* 10 Subjects
* 10 Items

```{r glmer01, message=F, warning=F}
# generate data
simdat <- data.frame(
  sub <- rep(paste0("Sub", 1:10), each = 20),
  cond <- rep(c(
    rep("Control", 10),
    rep("Test", 10))
    , 10),
  itm <- as.character(rep(1:10, 20))
) %>%
  dplyr::rename(Subject = 1,
                Condition = 2,
                Item = 3) %>%
  dplyr::mutate_if(is.character, factor)
# inspect
head(simdat, 15)
```


**EXERCISE**

Can you create a data set with 5 Subjects, 5 Items (each) for 4 Conditions?

```{r datex1, eval = F}

```

**Solution**

The first 20 rows of the data should look like shown below:

```{r datex2, echo = F}
# generate data
Items <- rep(rep(paste0("Item", 1:5), 4), 5)
Condition <- rep(rep(paste0("Condition", 1:4), each = 5), 5)
Subjects <- rep(paste0("Subject", 1:5), each = 20)
# combine into data frame
exdat <- data.frame(Subjects, Items, Condition)
# inspect
head(exdat, 20)
```




### Generating the model

Set effects and specify model parameters

* fixed effects (and intercept)

* random effects

* variance

```{r glmer02, message=F, warning=F}
# Intercept + slopes for fixed effects 
# (Group, Sentence_type, Word_order, and an interaction between Group * Sentence_type)
fixed <- c(.52, .52)
# Random intercepts for Sentence and ID
rand <- list(0.5, 0.1)
#rand <- list(0.5)
```

Generate model


```{r glmer03, message=F, warning=F}
m4 <- simr::makeGlmer(y ~ (1|Subject) + (1|Item) + Condition, 
                      family = "binomial", 
                      fixef=fixed, 
                      VarCorr=rand, 
                      data=simdat)
# inspect
sjPlot::tab_model(m4)
```

Extract power

```{r glmer04, message=F, warning=F}
# set seed for replicability
set.seed(12345)
# perform power analysis for present model
rsim_m4_c <- powerSim(m4, fixed("ConditionTest", "z"), nsim=20)
# inspect
rsim_m4_c
```

## Extending the data

### Adding Items

* 2 Conditions

* 10 Subjects

* 40 Items (from 10)

(each combination occurs once!)

```{r glmer06, message=F, warning=F}
m4_ai <- simr::extend(m4,
                      along="Item", 
                      n=40)
# inspect model
sjPlot::tab_model(m4_ai)
```


Plot power curve

```{r glmer08, message=F, warning=F}
pcurve_m4_ai_c <- powerCurve(m4_ai, 
                             fixed("ConditionTest", "z"), 
                             along = "Item",
                             nsim = 20,
                             breaks=seq(10, 40, 5), 
                             nsim=10)
# show plot
plot(pcurve_m4_ai_c)
```

### Adding subjects

* 2 Conditions

* 10 Items

* 40 Subjects (from 10)

(each combination occurs once!)

```{r glmer10, message=F, warning=F}
m4_as <- simr::extend(m4,
                      along="Subject", 
                      n=40)
# inspect model
sjPlot::tab_model(m4_as)
```


Plot power curve

```{r glmer12, message=F, warning=F}
pcurve_m4_as_c <- powerCurve(m4_as, 
                             fixed("ConditionTest", "z"), 
                             along = "Subject",
                             nsim = 20,
                             breaks=seq(10, 40, 5), 
                             nsim=10)
# show plot
plot(pcurve_m4_as_c)
```

How often does each *combination* occur?

**Only once!**

So, what if we increase the number of combinations? (**Repeated Measures Design!**)

* 2 Conditions

* 10 Items

* 10 Subjects 

(But each combination of item and subject now occurs 10 times!)

```{r glmer14, message=F, warning=F}
m4_asi_c <- extend(m4, 
                   within="Subject+Item", 
                   n=10)
# perform power calculation
pcurve_m4_asi_c <- powerCurve(m4_asi_c, 
                              fixed("ConditionTest", "z"), 
                              within="Subject+Item",
                              nsim = 20,
                              breaks=seq(2, 10, 2), 
                              nsim=10)
# show plot
plot(pcurve_m4_asi_c)
```

### Adding subjects and items

* 2 Conditions

* 30 Items (from 10)

* 30 Subjects (from 10)

(each combination occurs once!)

```{r glmer16, message=F, warning=F}
m4_as <- simr::extend(m4,
                      along="Subject", 
                      n=30)
m4_asi <- simr::extend(m4_as,
                      along="Item", 
                      n=30)
# inspect model
sjPlot::tab_model(m4_asi)
```


Plot power curve

*How many subjects do you need if you have 30 Items?*

```{r glmer18, message=F, warning=F}
pcurve_m4_asi_c <- powerCurve(m4_asi, 
                             fixed("ConditionTest", "z"), 
                             along = "Subject", 
                             breaks=seq(5, 30, 5), 
                             nsim=10)
# show plot
plot(pcurve_m4_asi_c)
```

# Post-Hoc Analyses

We now turn to post-hoc power analyses.

<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>NOTE</b><br>Power analysis have also been used post-hoc to test if the sample size of studies was sufficient to detect meaningful effects. However, such post-hoc power calculations where the target effect size comes from the data, give misleading results (Hoenig and Heisey 2001; Perugini, Gallucci, and Costantini 2018)  and should thus be treated with extreme care!</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>

We begin by adding a response variable to our data. In this case, we vary the response variable ti a higher likelihood of obtaining gazes in the *area of interests* (AOI) in the test condition.

```{r glmer20, message=F, warning=F}
simdat2 <- simdat %>%
  dplyr::arrange(Condition) %>%
  dplyr::mutate(
  AOI = c(sample(c("AOI", "NotAOI"), 100, replace = T, prob = c(.5, .5)),
           sample(c("AOI", "NotAOI"), 100, replace = T, prob = c(.7, .3)))
  ) %>%
  dplyr::mutate_if(is.character, factor)
# inspect
head(simdat2, 20)
```


Now that we have generated some data, we will fit a model to it and perform a power analysis on the observed effects. 


We will fit a first model to the data. Thus, in a first step, we load the `lme4` package to create a model, set a seed (to save the results and so that the results can be replicated), and then create an initial mixed-effects model.

```{r pwr1, message=F, warning=F}
# set seed for replicability
set.seed(12345)
# fit model
m5 <- glmer(AOI ~ (1|Subject) +(1|Item) + Condition, family="binomial", data=simdat2)
# inspect results
summary(m5)
```

We now check the effect sizes of the predictors in the model. We can do this by displaying the results of the model using the `tab_model` function from the `sjPlot` package.


```{r pwr5, message=F, warning=F}
# tabulate results
sjPlot::tab_model(m5)
```


Now, we perform power analysis on an observed effect. This analysis tells us how likely the model is to find an observed effect given the data.

***

<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>NOTE</b><br>We use a very low number of simulations (10) and we use the default z-test which is suboptimal for small samples (Bolker et al. 2009). In a proper study, you should increase the number of simulations (to at least 1000) and you should use a bootstrapping rather than a z-test (cf. Halekoh, Højsgaard, and others 2014). </p>
<p style='margin-left:1em;'>
</p></span>
</div>

***


*What is the probability of finding the observed effect given the data?*

```{r pwr7, results='hide', message = FALSE, warning=F}
# set seed for replicability
set.seed(12345)
# perform power analysis for present model
rsim0 <- powerSim(m5, 
                  fixed("ConditionTest", "z"), 
                  nsim=10)
# inspect results
rsim0
```


The results of the power analysis show that, given the data at hand, the model would have detected the effect of `Conidition:Test` with a probability of `r rsim0$x` percent. However, and as stated above, the results of such post-hoc power calculations (where the target effect size comes from the data) give misleading results (Hoenig and Heisey 2001)  and should thus be treated with extreme care!

# Fixing effects

We will set the effects that we obtained based on our "observed" data to check if, given the size of the data, we have enough power to detect a small effect of `Condition`. In a first step, we check the observed effects.

```{r pwr9, message=F, warning=F}
estimatesfixedeffects <- fixef(m5)
exp(estimatesfixedeffects)
```



We can see that the effect is already rather small which makes it very hard to detect an effect. We will now change the size of the effect of ConditionTest to represent a truly small effect, i.e. on the brink of being noise but being just strong enough to be considered small. In other words, we will set the effect so that its odds ratio is exactly 1.68.


```{r}
# set seed for replicability
set.seed(12345)
# perform power analysis for small effect
fixef(m5)["ConditionTest"] <- 0.519
estimatesfixedeffects <- fixef(m5)
exp(estimatesfixedeffects)
```

*What is the probability of finding a weak effect given the data?*

Perform power analysis

```{r pwr11, results='hide', message = FALSE, warning=F}
# set seed for replicability
set.seed(12345)
# perform power analysis for present model
rsim0 <- powerSim(m5, 
                  fixed("ConditionTest", "z"), 
                  nsim=20)
# inspect results
rsim0
```

# Power Analysis of Extended Data

We will now extend the data to see what sample size is needed to get to the 80 percent accuracy threshold. We begin by increasing the number of items from 10 to 30 to see if this would lead to a sufficient sample size.


```{r pwr13, results='hide', message = FALSE, warning=F}
# increase sample size
m5_ai <- extend(m5, 
             along="Item", 
             n=30)
# perform power simulation
rsim2 <- powerSim(m5_ai, 
                  fixed("ConditionTest", "z"), 
                  nsim=10)
# show results
rsim2
```



# Wrap-up and Resources

If you want to know more, please have a look/go at the following resources:

* [Power Analysis in R](https://slcladal.github.io/pwr.html)

* [SIMR: an R package for power analysis of generalized linear mixed models by simulation](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504)

* [Power analysis in Statistics with R](https://www.r-bloggers.com/2021/05/power-analysis-in-statistics-with-r/)

* [Power Analysis with simr](https://humburg.github.io/Power-Analysis/simr_power_analysis.html)

* [Parallelizing simr::powercurve() in R](https://www.r-bloggers.com/2021/07/parallelizing-simrpowercurve-in-r/)

That's all folks!

# Citation & Session Info 

Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *Introduction to Power Analysis with R - AcqVA Aurora workshop*. Tromsø: The Arctic University of Norway, Tromsø. url: https://github.com/MartinSchweinberger/AcqVA_PowerR_WS/acqvapwrws.html.



```{r fin}
sessionInfo()
```



# References 

Arnold, Benjamin F, Daniel R Hogan, John M Colford, and Alan E Hubbard. 2011. Simulation Methods to Estimate Design Power: An Overview for Applied Research. *British Medical Journal Medical Research Methodology* 11(1): 1–10.

Bolker, Benjamin M, Mollie E Brooks, Connie J Clark, Shane W Geange, John R Poulsen, M Henry H Stevens, and Jada-Simone S White. 2009. Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution. *Trends in Ecology & Evolution* 24(3): 127–35.

Bortz, J, GA Lienert, and K Boehnke. 1990. *Verteilungsfreie Methoden in der Biostatistik*. Berlin: Springer Verlag.

Champely, Stephane. 2020. *Pwr: Basic Functions for Power Analysis*. https://CRAN.R-project.org/package=pwr.

Chen, Henian, Patricia Cohen, and Sophie Chen. 2010. How Big Is a Big Odds Ratio? Interpreting the Magnitudes of Odds Ratios in Epidemiological Studies. *Communications in Statistics–Simulation and Computation* 39(4): 860–64.

Cohen, Jacob. 1988. *Statistical Power Analysis for the Behavioral Sciences*. Hillsdale, NJ: Erlbaum.

Field, Scott A, Patrick J O’Connor, Andrew J Tyre, and Hugh P Possingham. 2007. Making Monitoring Meaningful. *Australian Ecology* 32(5): 485–91.

Green, Peter, and Catriona J. MacLeod. 2016a. Simr: An R Package for Power Analysis of Generalised Linear Mixed Models by Simulation. *Methods in Ecology and Evolution* 7(4): 493–98. https://doi.org/10.1111/2041-210X.12504.

Green, Peter, and Catriona J MacLeod. 2016b. SIMR: An R Package for Power Analysis of Generalized Linear Mixed Models by Simulation. *Methods in Ecology and Evolution* 7(4): 493–98.

Halekoh, Ulrich, Søren Højsgaard, and others. 2014. A Kenward-Roger Approximation and Parametric Bootstrap Methods for Tests in Linear Mixed Models–the R Package Pbkrtest. *Journal of Statistical Software* 59(9): 1–30.

Hoenig, John M, and Dennis M Heisey. 2001. The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis. *The American Statistician* 55(1): 19–24.

Johnson, Paul CD, Sarah JE Barry, Heather M Ferguson, and Pie Müller. 2015. Power Analysis for Generalized Linear Mixed Models in Ecology and Evolution. *Methods in Ecology and Evolution* 6(2): 133–42.

Perugini, Marco, Marcello Gallucci, and Giulio Costantini. 2018. A Practical Primer to Power Analysis for Simple Experimental Designs. *International Review of Social Psychology* 31(1): 1–23.

